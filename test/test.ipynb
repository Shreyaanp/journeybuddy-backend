{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-llms-gemini\n",
      "  Using cached llama_index_llms_gemini-0.1.6-py3-none-any.whl.metadata (683 bytes)\n",
      "Collecting google-generativeai<0.4.0,>=0.3.2 (from llama-index-llms-gemini)\n",
      "  Using cached google_generativeai-0.3.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting llama-index-core<0.11.0,>=0.10.11.post1 (from llama-index-llms-gemini)\n",
      "  Using cached llama_index_core-0.10.26-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting pillow<11.0.0,>=10.2.0 (from llama-index-llms-gemini)\n",
      "  Using cached pillow-10.2.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
      "Collecting google-ai-generativelanguage==0.4.0 (from google-generativeai<0.4.0,>=0.3.2->llama-index-llms-gemini)\n",
      "  Using cached google_ai_generativelanguage-0.4.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting google-auth (from google-generativeai<0.4.0,>=0.3.2->llama-index-llms-gemini)\n",
      "  Using cached google_auth-2.29.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-api-core (from google-generativeai<0.4.0,>=0.3.2->llama-index-llms-gemini)\n",
      "  Using cached google_api_core-2.18.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typing-extensions (from google-generativeai<0.4.0,>=0.3.2->llama-index-llms-gemini)\n",
      "  Using cached typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting protobuf (from google-generativeai<0.4.0,>=0.3.2->llama-index-llms-gemini)\n",
      "  Using cached protobuf-5.26.1-cp37-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting tqdm (from google-generativeai<0.4.0,>=0.3.2->llama-index-llms-gemini)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.4.0->google-generativeai<0.4.0,>=0.3.2->llama-index-llms-gemini)\n",
      "  Using cached proto_plus-1.23.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf (from google-generativeai<0.4.0,>=0.3.2->llama-index-llms-gemini)\n",
      "  Using cached protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Collecting PyYAML>=6.0.1 (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting SQLAlchemy>=1.4.49 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached SQLAlchemy-2.0.29-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.6 (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached aiohttp-3.9.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting dataclasses-json (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached dataclasses_json-0.6.4-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting fsspec>=2023.5.0 (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting llamaindex-py-client<0.2.0,>=0.1.15 (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached llamaindex_py_client-0.1.15-py3-none-any.whl.metadata (762 bytes)\n",
      "Collecting nest-asyncio<2.0.0,>=1.5.8 (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting networkx>=3.0 (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting nltk<4.0.0,>=3.8.1 (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting numpy (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting openai>=1.1.0 (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached openai-1.14.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting pandas (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached pandas-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting requests>=2.31.0 (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tenacity<9.0.0,>=8.2.0 (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting tiktoken>=0.3.3 (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached tiktoken-0.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting wrapt (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached wrapt-1.16.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached frozenlist-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached multidict-6.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached yarl-1.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core->google-generativeai<0.4.0,>=0.3.2->llama-index-llms-gemini)\n",
      "  Using cached googleapis_common_protos-1.63.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth->google-generativeai<0.4.0,>=0.3.2->llama-index-llms-gemini)\n",
      "  Using cached cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth->google-generativeai<0.4.0,>=0.3.2->llama-index-llms-gemini)\n",
      "  Using cached pyasn1_modules-0.4.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth->google-generativeai<0.4.0,>=0.3.2->llama-index-llms-gemini)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting pydantic>=1.10 (from llamaindex-py-client<0.2.0,>=0.1.15->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached pydantic-2.6.4-py3-none-any.whl.metadata (85 kB)\n",
      "Collecting anyio (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached anyio-4.3.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting certifi (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting httpcore==1.* (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting idna (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting sniffio (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting click (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached regex-2023.12.25-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached greenlet-3.0.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached marshmallow-3.21.1-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.4.0->google-generativeai<0.4.0,>=0.3.2->llama-index-llms-gemini)\n",
      "  Using cached grpcio-1.62.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-ai-generativelanguage==0.4.0->google-generativeai<0.4.0,>=0.3.2->llama-index-llms-gemini)\n",
      "  Using cached grpcio_status-1.62.1-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting packaging>=17.0 (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached packaging-24.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth->google-generativeai<0.4.0,>=0.3.2->llama-index-llms-gemini)\n",
      "  Using cached pyasn1-0.6.0-py2.py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.15->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.16.3 (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.15->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached pydantic_core-2.16.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-gemini)\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Using cached llama_index_llms_gemini-0.1.6-py3-none-any.whl (4.7 kB)\n",
      "Using cached google_generativeai-0.3.2-py3-none-any.whl (146 kB)\n",
      "Using cached google_ai_generativelanguage-0.4.0-py3-none-any.whl (598 kB)\n",
      "Using cached llama_index_core-0.10.26-py3-none-any.whl (15.4 MB)\n",
      "Using cached pillow-10.2.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "Using cached aiohttp-3.9.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Using cached fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "Using cached google_api_core-2.18.0-py3-none-any.whl (138 kB)\n",
      "Using cached google_auth-2.29.0-py2.py3-none-any.whl (189 kB)\n",
      "Using cached llamaindex_py_client-0.1.15-py3-none-any.whl (119 kB)\n",
      "Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Using cached nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\n",
      "Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached openai-1.14.3-py3-none-any.whl (262 kB)\n",
      "Using cached protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Using cached PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached SQLAlchemy-2.0.29-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Using cached tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Using cached tiktoken-0.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Using cached typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached wrapt-1.16.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "Using cached dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "Using cached pandas-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached anyio-4.3.0-py3-none-any.whl (85 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (140 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached frozenlist-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (272 kB)\n",
      "Using cached googleapis_common_protos-1.63.0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached greenlet-3.0.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (620 kB)\n",
      "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Using cached marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
      "Using cached multidict-6.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (128 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Using cached proto_plus-1.23.0-py3-none-any.whl (48 kB)\n",
      "Using cached pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "Using cached pydantic-2.6.4-py3-none-any.whl (394 kB)\n",
      "Using cached pydantic_core-2.16.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached regex-2023.12.25-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Using cached yarl-1.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (328 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Using cached grpcio-1.62.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n",
      "Using cached grpcio_status-1.62.1-py3-none-any.whl (14 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached packaging-24.0-py3-none-any.whl (53 kB)\n",
      "Using cached pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
      "Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: pytz, dirtyjson, wrapt, urllib3, tzdata, typing-extensions, tqdm, tenacity, sniffio, six, regex, PyYAML, pyasn1, protobuf, pillow, packaging, numpy, networkx, nest-asyncio, mypy-extensions, multidict, joblib, idna, h11, grpcio, greenlet, fsspec, frozenlist, distro, click, charset-normalizer, certifi, cachetools, attrs, annotated-types, yarl, typing-inspect, SQLAlchemy, rsa, requests, python-dateutil, pydantic-core, pyasn1-modules, proto-plus, nltk, marshmallow, httpcore, googleapis-common-protos, deprecated, anyio, aiosignal, tiktoken, pydantic, pandas, httpx, grpcio-status, google-auth, dataclasses-json, aiohttp, openai, llamaindex-py-client, google-api-core, llama-index-core, google-ai-generativelanguage, google-generativeai, llama-index-llms-gemini\n",
      "Successfully installed PyYAML-6.0.1 SQLAlchemy-2.0.29 aiohttp-3.9.3 aiosignal-1.3.1 annotated-types-0.6.0 anyio-4.3.0 attrs-23.2.0 cachetools-5.3.3 certifi-2024.2.2 charset-normalizer-3.3.2 click-8.1.7 dataclasses-json-0.6.4 deprecated-1.2.14 dirtyjson-1.0.8 distro-1.9.0 frozenlist-1.4.1 fsspec-2024.3.1 google-ai-generativelanguage-0.4.0 google-api-core-2.18.0 google-auth-2.29.0 google-generativeai-0.3.2 googleapis-common-protos-1.63.0 greenlet-3.0.3 grpcio-1.62.1 grpcio-status-1.62.1 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 idna-3.6 joblib-1.3.2 llama-index-core-0.10.26 llama-index-llms-gemini-0.1.6 llamaindex-py-client-0.1.15 marshmallow-3.21.1 multidict-6.0.5 mypy-extensions-1.0.0 nest-asyncio-1.6.0 networkx-3.2.1 nltk-3.8.1 numpy-1.26.4 openai-1.14.3 packaging-24.0 pandas-2.2.1 pillow-10.2.0 proto-plus-1.23.0 protobuf-4.25.3 pyasn1-0.6.0 pyasn1-modules-0.4.0 pydantic-2.6.4 pydantic-core-2.16.3 python-dateutil-2.9.0.post0 pytz-2024.1 regex-2023.12.25 requests-2.31.0 rsa-4.9 six-1.16.0 sniffio-1.3.1 tenacity-8.2.3 tiktoken-0.6.0 tqdm-4.66.2 typing-extensions-4.10.0 typing-inspect-0.9.0 tzdata-2024.1 urllib3-2.2.1 wrapt-1.16.0 yarl-1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-llms-gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_pipeline import (\n",
    "    QueryPipeline as QP,\n",
    "    Link,\n",
    "    InputComponent,\n",
    ")\n",
    "from llama_index.core.query_engine.pandas import PandasInstructionParser\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.core import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df= pd.read_csv(\"./hotel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "GOOGLE_API_KEY = \"AIzaSyCMSTWcXnJ1TQwGk-SeqORKnYnIhJm2FdU\"  # add your GOOGLE API key here\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_str = (\n",
    "    \"1. Convert the query to executable Python code using Pandas.\\n\"\n",
    "    \"2. The final line of code should be a Python expression that can be called with the `eval()` function.\\n\"\n",
    "    \"3. The code should represent a solution to the query.\\n\"\n",
    "    \"4. PRINT ONLY THE EXPRESSION.\\n\"\n",
    "    \"5. Do not quote the expression.\\n\"\n",
    ")\n",
    "\n",
    "pandas_prompt_str = (\n",
    "    \"You are working with a pandas dataframe in Python.\\n\"\n",
    "    \"The name of the dataframe is `df`.\\n\"\n",
    "    \"This is the result of `print(df.head())`:\\n\"\n",
    "    \"{df_str}\\n\\n\"\n",
    "    \"Follow these instructions:\\n\"\n",
    "    \"{instruction_str}\\n\"\n",
    "    \"Query: {query_str}\\n\\n\"\n",
    "    \"Expression:\"\n",
    ")\n",
    "response_synthesis_prompt_str = (\n",
    "    \"Given an input question, synthesize a response from the query results.\\n\"\n",
    "    \"Query: {query_str}\\n\\n\"\n",
    "    \"Pandas Instructions (optional):\\n{pandas_instructions}\\n\\n\"\n",
    "    \"Pandas Output: {pandas_output}\\n\\n\"\n",
    "    \"Response: \"\n",
    ")\n",
    "\n",
    "pandas_prompt = PromptTemplate(pandas_prompt_str).partial_format(\n",
    "    instruction_str=instruction_str, df_str=df.head(5)\n",
    ")\n",
    "pandas_output_parser = PandasInstructionParser(df)\n",
    "response_synthesis_prompt = PromptTemplate(response_synthesis_prompt_str)\n",
    "llm = Gemini(model=\"models/gemini-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "qp = QP(\n",
    "    modules={\n",
    "        \"input\": InputComponent(),\n",
    "        \"pandas_prompt\": pandas_prompt,\n",
    "        \"llm1\": llm,\n",
    "        \"pandas_output_parser\": pandas_output_parser,\n",
    "        \"response_synthesis_prompt\": response_synthesis_prompt,\n",
    "        \"llm2\": llm,\n",
    "    },\n",
    "    verbose=True,\n",
    ")\n",
    "qp.add_chain([\"input\", \"pandas_prompt\", \"llm1\", \"pandas_output_parser\"])\n",
    "qp.add_links(\n",
    "    [\n",
    "        Link(\"input\", \"response_synthesis_prompt\", dest_key=\"query_str\"),\n",
    "        Link(\n",
    "            \"llm1\", \"response_synthesis_prompt\", dest_key=\"pandas_instructions\"\n",
    "        ),\n",
    "        Link(\n",
    "            \"pandas_output_parser\",\n",
    "            \"response_synthesis_prompt\",\n",
    "            dest_key=\"pandas_output\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "# add link from response synthesis prompt to llm2\n",
    "qp.add_link(\"response_synthesis_prompt\", \"llm2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module input with input: \n",
      "query_str: my budget is 3000 and there are 3 people there please suggest me some hotels ?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module pandas_prompt with input: \n",
      "query_str: my budget is 3000 and there are 3 people there please suggest me some hotels ?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm1 with input: \n",
      "messages: You are working with a pandas dataframe in Python.\n",
      "The name of the dataframe is `df`.\n",
      "This is the result of `print(df.head())`:\n",
      "                                           hotelName  \\\n",
      "0  Fortune Resor...\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module pandas_output_parser with input: \n",
      "input: assistant: df[(df['budget'] <= 3000) & (df['accommodationCapacity'] >= 3)]\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module response_synthesis_prompt with input: \n",
      "query_str: my budget is 3000 and there are 3 people there please suggest me some hotels ?\n",
      "pandas_instructions: assistant: df[(df['budget'] <= 3000) & (df['accommodationCapacity'] >= 3)]\n",
      "pandas_output:                      hotelName  \\\n",
      "13          The Majestic Crown   \n",
      "14           Astoria Residency   \n",
      "15      al Woodlands Residency   \n",
      "16          Ponmari Residencyy   \n",
      "19   FabExpress Ooty Residency...\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm2 with input: \n",
      "messages: Given an input question, synthesize a response from the query results.\n",
      "Query: my budget is 3000 and there are 3 people there please suggest me some hotels ?\n",
      "\n",
      "Pandas Instructions (optional):\n",
      "df[(df['bu...\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = qp.run(\n",
    "    query_str=\"my budget is 3000 and there are 3 people there please suggest me some hotels ?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but based on the information provided, I cannot find any hotels that meet your criteria.\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module input with input: \n",
      "query_str: We're a group of 3 friends . Our budget is 7000, and we're looking for a hotel. you can divide the rooms to break down the structure like if in data there are rooms to accomodate 2 people then you can...\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module pandas_prompt with input: \n",
      "query_str: We're a group of 3 friends . Our budget is 7000, and we're looking for a hotel. you can divide the rooms to break down the structure like if in data there are rooms to accomodate 2 people then you can...\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm1 with input: \n",
      "messages: You are working with a pandas dataframe in Python.\n",
      "The name of the dataframe is `df`.\n",
      "This is the result of `print(df.head())`:\n",
      "                                           hotelName  \\\n",
      "0  Fortune Resor...\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module pandas_output_parser with input: \n",
      "input: assistant: df[(df['accommodationCapacity'] == 3) | ((df['accommodationCapacity'] == 2) & (df['budget'] <= 7000))][['hotelName', 'budget']]\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module response_synthesis_prompt with input: \n",
      "query_str: We're a group of 3 friends . Our budget is 7000, and we're looking for a hotel. you can divide the rooms to break down the structure like if in data there are rooms to accomodate 2 people then you can...\n",
      "pandas_instructions: assistant: df[(df['accommodationCapacity'] == 3) | ((df['accommodationCapacity'] == 2) & (df['budget'] <= 7000))][['hotelName', 'budget']]\n",
      "pandas_output:                                              hotelName  budget\n",
      "3                            Aakash Rooms And Cottages    5719\n",
      "4                                     Hotel Silver Oak    8479\n",
      "6          ...\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm2 with input: \n",
      "messages: Given an input question, synthesize a response from the query results.\n",
      "Query: We're a group of 3 friends . Our budget is 7000, and we're looking for a hotel. you can divide the rooms to break down the...\n",
      "\n",
      "\u001b[0massistant: **Hotel Combinations for 3 Friends with a Budget of 7000:**\n",
      "\n",
      "* **Aakash Rooms And Cottages:** 2 rooms for 3 people, total cost: 5719\n",
      "* **Meadows Residency:** 2 rooms for 3 people, total cost: 5005\n",
      "* **FabHotel Sholas Residency I - Hotel on Shoram ...:** 2 rooms for 3 people, total cost: 5807\n",
      "* **OYO 22924 Gv Residency:** 2 rooms for 3 people, total cost: 2415\n",
      "* **Hotel Shiv Shanti Grand:** 2 rooms for 3 people, total cost: 1069\n",
      "* **Sabol Holiday Resorts:** 2 rooms for 3 people, total cost: 1069\n"
     ]
    }
   ],
   "source": [
    "response = qp.run(\n",
    "    query_str=\"We're a group of 3 friends . Our budget is 7000, and we're looking for a hotel. you can divide the rooms to break down the structure like if in data there are rooms to accomodate 2 people then you can suggest combo. like take two room in the same hotel and the cost together should be in and around the budget. do not display nothing \",\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I cannot provide any hotel suggestions based on the given query results as the DataFrame is empty.\n"
     ]
    }
   ],
   "source": [
    "print(response.message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
